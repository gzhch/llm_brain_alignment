{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1d4a23a-44f6-4ced-b91d-e73e99277654",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-01-31T13:22:09.441887Z",
     "shell.execute_reply.started": "2024-01-31T13:22:03.559337Z",
     "to_execute": "2024-01-31T13:22:03.542Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f5772f4c0a0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import json\n",
    "import argparse\n",
    "import random\n",
    "import scipy\n",
    "import config\n",
    "from LLAMA import LLAMA\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, pipeline\n",
    "import utils_llama.activation as ana\n",
    "import scipy\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import datasets\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "# 设置随机种子以便结果可重复\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.empty_cache()\n",
    "torch.set_grad_enabled(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f550610-8888-401d-85ff-3e270a73c4aa",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-01-31T13:28:32.728269Z",
     "shell.execute_reply.started": "2024-01-31T13:22:24.400203Z",
     "to_execute": "2024-01-31T13:22:24.276Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 21:22:42,823 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [05:48<00:00, 174.38s/it]\n"
     ]
    }
   ],
   "source": [
    "class ARGS:\n",
    "    def __init__(self):\n",
    "        self.subject = 'S1'\n",
    "        self.gpt = 'perceived'\n",
    "        self.sessions = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 18, 20]\n",
    "        self.layer = 17\n",
    "        self.layer2 = 18\n",
    "        self.act_name = 'ffn_gate'\n",
    "        self.window = 15\n",
    "        self.chunk = 4\n",
    "\n",
    "args = ARGS()\n",
    "\n",
    "model_dir = '/ossfs/workspace/nas/gzhch/data/models/Llama-2-7b-hf'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir, \n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16,\n",
    ").eval()\n",
    "\n",
    "# model = None\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "## load cached llm act if possible\n",
    "cache_dir = '/ossfs/workspace/nas/gzhch/data/cache'\n",
    "llama = LLAMA(model, tokenizer, cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27da0c2e-441d-4f5c-9429-96297271cef3",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-01-31T13:30:55.897139Z",
     "shell.execute_reply.started": "2024-01-31T13:30:55.693016Z",
     "to_execute": "2024-01-31T13:30:55.580Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "def load_data(task_name, n_shot=1, seed=42):\n",
    "    data_dirs = {\n",
    "        'xsum' : '/ossfs/workspace/nas/gzhch/data/datasets/xsum',\n",
    "        'gsm8k' : '/ossfs/workspace/nas/gzhch/data/datasets/gsm8k',\n",
    "        'alpaca' : '/ossfs/workspace/nas/gzhch/data/datasets/alpaca',\n",
    "        'wmt' : '/ossfs/workspace/nas/gzhch/data/datasets/wmt14_de-en_test',\n",
    "        'wikitext2' : '/ossfs/workspace/nas/gzhch/data/datasets/wikitext-2-v1',\n",
    "        'wikitext_dense' : '/ossfs/workspace/nas/gzhch/data/datasets/wikitext-2-v1',\n",
    "        'wikitext_eval' : '/ossfs/workspace/nas/gzhch/data/datasets/wikitext-2-v1',\n",
    "        'cross_language' : '/ossfs/workspace/nas/gzhch/data/datasets/wmt14_de-en_test',\n",
    "    }\n",
    "    if task_name == 'gsm8k':\n",
    "        dataset = datasets.load_dataset(data_dirs[task_name])\n",
    "    elif task_name == 'wikitext2':\n",
    "        dataset = datasets.load_from_disk(data_dirs[task_name])\n",
    "        dataset = dataset['train'].filter(lambda x: len(x['text'])>100) \n",
    "        dataset = dataset.select(random.sample(range(len(dataset)), 1000))\n",
    "\n",
    "    elif task_name == 'wikitext_eval':\n",
    "        dataset = datasets.load_from_disk(data_dirs[task_name])\n",
    "        dataset = dataset['test'].filter(lambda x: len(x['text'])>100) \n",
    "\n",
    "    elif task_name == 'cross_language':\n",
    "        dataset = datasets.load_from_disk(data_dirs[task_name])\n",
    "        de_data = dataset.map(lambda e: dict(text=e['translation']['de']))\n",
    "        en_data = dataset.map(lambda e: dict(text=e['translation']['en']))\n",
    "        return en_data, de_data\n",
    "\n",
    "    elif task_name == 'wikitext_dense':\n",
    "        def tokenize_texts(examples):\n",
    "            tokenized_inputs = tokenizer(examples[\"text\"])\n",
    "            return tokenized_inputs\n",
    "\n",
    "        def group_texts(examples):\n",
    "            # Concatenate all texts.\n",
    "            max_length = 1024\n",
    "            concatenated_examples = {k: list(chain(*examples[k])) for k in ['input_ids']}\n",
    "            total_length = len(concatenated_examples['input_ids'])\n",
    "            # print(len(concatenated_examples['input_ids']), '\\n')\n",
    "            # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "            # customize this part to your needs.\n",
    "            if total_length >= max_length:\n",
    "                total_length = (total_length // max_length) * max_length\n",
    "            # else:\n",
    "                # print('aaa')\n",
    "            # Split by chunks of max_len.\n",
    "            # result = {\n",
    "            #     k: [t[i : i + max_length] for i in range(0, total_length, max_length)]\n",
    "            #     for k, t in concatenated_examples.items()\n",
    "            # }\n",
    "            result = {'input_ids': [concatenated_examples['input_ids'][i : i + max_length] for i in range(0, total_length, max_length)]}\n",
    "            return result\n",
    "\n",
    "        dataset = datasets.load_from_disk(data_dirs[task_name])\n",
    "        dataset = dataset.map(tokenize_texts, batched=True, num_proc=4)\n",
    "        dataset = dataset.map(group_texts, batched=True, num_proc=4, remove_columns=['text', 'attention_mask'])\n",
    "        dataset['train'] = dataset['train'].shuffle(seed=seed)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# 创建一个简单的两层全连接神经网络\n",
    "class Projector(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Projector, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.act = nn.SiLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.act(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.act(out)\n",
    "        return out\n",
    "\n",
    "class LinearProjector(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearProjector, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        # self.act = nn.SiLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        # out = self.act(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval(x, y, net):\n",
    "    output = net(x)\n",
    "    loss = criterion(output, y)\n",
    "    return loss\n",
    "\n",
    "def deduplication(data):\n",
    "    tokens = data['context'][:, 5]\n",
    "    unique_tokens = []\n",
    "    unique_token_ids = []\n",
    "    for idx in range(len(tokens)):\n",
    "        if tokens[idx] not in unique_tokens:\n",
    "            unique_tokens.append(tokens[idx])\n",
    "            unique_token_ids.append(idx)\n",
    "    random.shuffle(unique_token_ids)\n",
    "    ids = unique_token_ids\n",
    "\n",
    "    return {k : v[ids] for k, v in data.items()}\n",
    "\n",
    "def train(net, train_set, stim_neurons=None, resp_neurons=None, max_step=100000):\n",
    "    logs = []\n",
    "    # layer1, layer2 = 10, 15\n",
    "    total_batch = len(train_set) // batch_size\n",
    "\n",
    "    total_batch = min(total_batch, max_step)\n",
    "    for b in range(total_batch):\n",
    "        input_ids = train_set[b * batch_size: (b + 1) * batch_size]['input_ids']\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        input = dict(input_ids=input_ids, attention_mask=torch.ones(input_ids.shape))\n",
    "        with torch.no_grad():\n",
    "            res = llama.get_neuron_activation_and_loss(input)\n",
    "\n",
    "        if stim_neurons is not None:\n",
    "            X = res['ffn_gate'][:, layer1, stim_neurons].cuda().float()\n",
    "        else:\n",
    "            X = res['ffn_gate'][:, layer1, :].cuda().float()\n",
    "        if resp_neurons is not None:\n",
    "            Y = res['ffn_gate'][:, layer2, resp_neurons].cuda().float()\n",
    "        else:\n",
    "            Y = res['ffn_gate'][:, layer2, :].cuda().float()\n",
    "\n",
    "        output = net(X)\n",
    "        loss = criterion(output, Y)\n",
    "        \n",
    "        optimizer.zero_grad() \n",
    "        (loss * output.shape[1]).backward()        \n",
    "        optimizer.step()       \n",
    "        \n",
    "        if (b+1) % 1 == 0:\n",
    "            eval_loss = eval(test_X.cuda(), test_Y.cuda(), net).item()\n",
    "            print(f'Epoch [{b+1}/{total_batch}], Train Loss: {loss.item():.6f}, Eval Loss: {eval_loss:.6f}')\n",
    "            logs.append(f'Epoch [{b+1}/{total_batch}], Train Loss: {loss.item():.6f}, Eval Loss: {eval_loss:.6f}')\n",
    "    return logs\n",
    "\n",
    "def evaluate_ppl(eval_data, model, fake_ffn=None, num_of_batch=3, **forwrd_args):\n",
    "    ppls = []\n",
    "    batch_size = 100\n",
    "    for b in range(num_of_batch):\n",
    "        input = tokenizer(eval_data['text'][b * batch_size: (b + 1) * batch_size], padding='longest', return_tensors='pt')\n",
    "        result = ana.custom_forward(model, input['input_ids'].cuda(), inspect_acts=['ffn_gate'], fake_ffn=fake_ffn, **forwrd_args)\n",
    "        logits = result['logits']\n",
    "        labels = input['input_ids']\n",
    "        input_ids = input['input_ids'][:, :-1]\n",
    "\n",
    "        # calculate loss\n",
    "        shift_logits = logits[..., :-1, :].contiguous().view(-1, 32000)\n",
    "        shift_labels = labels[..., 1:].contiguous().view(-1)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(reduce=False)\n",
    "        loss = loss_fct(shift_logits, shift_labels).view(labels.shape[0], -1)\n",
    "        t = (loss * input['attention_mask'][:, :-1]).sum(dim=1)/input['attention_mask'].sum(dim=1)\n",
    "        ppls += torch.exp(t).tolist()\n",
    "    ppl = torch.nan_to_num(torch.tensor(ppls)).mean().tolist()\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d37792b-f1c1-4dfe-a08d-08a7488dacb1",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-01-31T13:38:38.918038Z",
     "shell.execute_reply.started": "2024-01-31T13:38:38.752401Z",
     "to_execute": "2024-01-31T13:38:38.629Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "def get_log(layer1, layer2):\n",
    "\n",
    "    with open(f'/ossfs/workspace/cache_v2/{layer1}-{layer2}.txt', 'r') as f:\n",
    "        logs = f.readlines()\n",
    "    return logs\n",
    "\n",
    "\n",
    "def f(layer1, layer2):\n",
    "\n",
    "    with open(f'/ossfs/workspace/cache_v2/{layer1}-{layer2}.txt', 'r') as f:\n",
    "        logs = f.readlines()\n",
    "    # return logs\n",
    "\n",
    "    stim_neurons = None\n",
    "    resp_neurons = None\n",
    "\n",
    "    ### get text set\n",
    "    test_X, test_Y = [], []\n",
    "    if stim_neurons is not None:\n",
    "        test_X = test_data['ffn_gate'][:, layer1, stim_neurons].cuda().half()\n",
    "    else:\n",
    "        test_X = test_data['ffn_gate'][:, layer1, :].cuda().half()\n",
    "    if resp_neurons is not None:\n",
    "        test_Y = test_data['ffn_gate'][:, layer2, resp_neurons].cuda().half()\n",
    "    else:\n",
    "        test_Y = test_data['ffn_gate'][:, layer2, :].cuda().half()\n",
    "\n",
    "    save_path = f'/ossfs/workspace/cache_v2/net_{layer1}_{layer2}.pt'\n",
    "    # save_path = f'/ossfs/workspace/nas/gzhch/data/cache/llama-7b/net_{layer1}_{layer2}.pt'\n",
    "    # if not os.path.exists(save_path):\n",
    "    #     save_path = f'/ossfs/workspace/nas/gzhch/data/cache/llama-7b/net_{layer1}_{layer2}.pt'\n",
    "\n",
    "    net = torch.load(save_path).half()\n",
    "\n",
    "    pred = net(test_X)\n",
    "\n",
    "    th = 0.6\n",
    "    pred = net(test_X)\n",
    "    delta = pred - test_Y\n",
    "    ids = torch.nonzero(((delta.std(dim=0)) / test_Y.std(dim=0)).abs() < th).squeeze()\n",
    "    return pred, test_Y, logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eefdbbe-d803-4a54-abce-9fb2899f8f04",
   "metadata": {
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "wiki_data = load_data('wikitext_dense')\n",
    "\n",
    "# get test data once and for all\n",
    "batch_size = 10\n",
    "test_data = []\n",
    "for b in range(5):\n",
    "    input_ids = wiki_data['validation'][b * batch_size: (b + 1) * batch_size]['input_ids']\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    input = dict(input_ids=input_ids, attention_mask=torch.ones(input_ids.shape))\n",
    "    with torch.no_grad():\n",
    "        res = llama.get_neuron_activation_and_loss(input)\n",
    "        test_data.append(res)\n",
    "test_data = {k: torch.cat([i[k] for i in test_data]) for k in test_data[0].keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "284552f9-ba4d-42ff-8c70-f073d79ea308",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-01-29T11:51:19.248203Z",
     "shell.execute_reply.started": "2024-01-29T11:51:04.625856Z",
     "to_execute": "2024-01-29T11:51:04.567Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "## get log\n",
    "results = [[] for _ in range(0, 32, 2)]\n",
    "for i, layer1 in enumerate(range(0, 32, 2)):\n",
    "    for j, layer2 in enumerate(range(0, 32, 2)):\n",
    "        logs = get_log(layer1, layer2)\n",
    "        results[i].append(float(logs[-1].split()[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "679b938d-69cc-4cc7-aae2-78bacfa1ddd3",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-01-30T13:45:33.880950Z",
     "shell.execute_reply.started": "2024-01-30T13:45:05.079224Z",
     "to_execute": "2024-01-30T13:45:05.094Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "layer1 = 8\n",
    "layer2 = 10\n",
    "pred, test_Y, _ = f(layer1, layer2)\n",
    "\n",
    "neuron_pearson = []\n",
    "for i in range(pred.shape[1]):\n",
    "    stat = scipy.stats.pearsonr(pred[:, i].cpu().detach(), test_Y[:, i].cpu().detach())\n",
    "    neuron_pearson.append(stat.statistic)\n",
    "neuron_pearson = torch.tensor(neuron_pearson)\n",
    "\n",
    "neuron_std = (pred - test_Y).std(dim=0).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "5265c1b0-a76b-456c-97a4-ce1d2e75aaac",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-01-31T12:57:55.580314Z",
     "shell.execute_reply.started": "2024-01-31T12:57:55.548915Z",
     "to_execute": "2024-01-31T12:57:55.392Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "# indices = neuron_pearson.topk(100, largest=True).indices.cpu()\n",
    "indices = neuron_std.topk(10, largest=True).indices.cpu()\n",
    "neuron_id = indices\n",
    "neuron_weight = model.model.layers[layer2].mlp.down_proj.weight[:, neuron_id]\n",
    "lm_head = model.lm_head.weight\n",
    "logit_contribution = torch.matmul(lm_head, neuron_weight.to(lm_head.device)).transpose(0, 1)\n",
    "logits = logit_contribution.topk(100, dim=1).indices\n",
    "# tokenizer.convert_ids_to_tokens(logit_contribution.topk(10, dim=0).indices.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3139bc7-d611-47bc-bb99-58c4a5acb891",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-01-31T13:38:58.003922Z",
     "shell.execute_reply.started": "2024-01-31T13:38:57.718365Z",
     "to_execute": "2024-01-31T13:38:57.594Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/ossfs/workspace/cache_v2/8-10.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m layer1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m      2\u001b[0m layer2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 3\u001b[0m pred, test_Y, _ \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer2\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m, in \u001b[0;36mf\u001b[0;34m(layer1, layer2)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(layer1, layer2):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/ossfs/workspace/cache_v2/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlayer1\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlayer2\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     11\u001b[0m         logs \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# return logs\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/ossfs/workspace/cache_v2/8-10.txt'"
     ]
    }
   ],
   "source": [
    "layer1 = 8\n",
    "layer2 = 10\n",
    "pred, test_Y, _ = f(layer1, layer2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "5b9694fb-3222-4629-b61c-c96d0e314e64",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-01-31T13:17:26.205592Z",
     "shell.execute_reply.started": "2024-01-31T13:17:26.054067Z",
     "to_execute": "2024-01-31T13:17:25.864Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-28.7031, dtype=torch.float16)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = 20\n",
    "neurons = test_data['ffn_gate'][:, layer, :]\n",
    "neurons.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "43ef878f-c3bc-40e9-8272-4e5181eb73ae",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "",
     "shell.execute_reply.started": "2024-01-31T13:18:33.958824Z",
     "to_execute": "2024-01-31T13:18:33.771Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "with open('/ossfs/workspace/test_data.pkl', 'wb') as f:\n",
    "    pickle.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "9c880746-d273-4c7b-8c0b-57a82ef16ff1",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-01-31T12:23:41.899593Z",
     "shell.execute_reply.started": "2024-01-31T12:22:57.756473Z",
     "to_execute": "2024-01-31T12:22:57.672Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "layer1 = 4\n",
    "layer2 = 6\n",
    "en_data, de_data = load_data('cross_language')\n",
    "eval_data = en_data\n",
    "fake_ffn = ana.FFNProjector(layer1, layer2, torch.load(f'/ossfs/workspace/cache_v2/net_{layer1}_{layer2}.pt'))\n",
    "\n",
    "batch = [9]\n",
    "ppls = []\n",
    "batch_size = 100\n",
    "for b in range(10):\n",
    "    input = tokenizer(eval_data['text'][b * batch_size: (b + 1) * batch_size], padding='longest', return_tensors='pt')\n",
    "    result = ana.custom_forward(model, input['input_ids'].cuda(), inspect_acts=['ffn_gate'], fake_ffn=fake_ffn)\n",
    "    logits = result['logits']\n",
    "    labels = input['input_ids']\n",
    "    input_ids = input['input_ids'][:, :-1]\n",
    "\n",
    "    # calculate loss\n",
    "    shift_logits = logits[..., :-1, :].contiguous().view(-1, 32000)\n",
    "    shift_labels = labels[..., 1:].contiguous().view(-1)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(reduce=False)\n",
    "    loss = loss_fct(shift_logits, shift_labels).view(labels.shape[0], -1)\n",
    "    # print(loss)\n",
    "    t = (loss * input['attention_mask'][:, :-1]).sum(dim=1)/input['attention_mask'].sum(dim=1)\n",
    "    ppls += torch.exp(t).tolist()\n",
    "ppl = torch.nan_to_num(torch.tensor(ppls)).mean().tolist()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "5b7eba6b-e0af-4063-8846-dfe56cd1ad7d",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-01-31T12:23:53.894727Z",
     "shell.execute_reply.started": "2024-01-31T12:23:53.863681Z",
     "to_execute": "2024-01-31T12:23:53.777Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(42.5913)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(ppls).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c670787e-cb42-4002-bacf-627207095ec1",
   "metadata": {
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
