{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1d4a23a-44f6-4ced-b91d-e73e99277654",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-02-02T07:40:29.595204Z",
     "shell.execute_reply.started": "2024-02-02T07:40:21.675960Z",
     "to_execute": "2024-02-02T07:40:21.604Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-02-02 15:40:25,727 - datasets - INFO - PyTorch version 2.1.0a0+29c30b1 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-02 15:40:28,682] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f9e15767940>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import json\n",
    "import argparse\n",
    "import random\n",
    "import scipy\n",
    "import config\n",
    "from LLAMA import LLAMA\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, pipeline\n",
    "import utils_llama.activation as ana\n",
    "import scipy\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import datasets\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from itertools import chain, product\n",
    "\n",
    "\n",
    "# 设置随机种子以便结果可重复\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.empty_cache()\n",
    "torch.set_grad_enabled(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f550610-8888-401d-85ff-3e270a73c4aa",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-02-02T07:40:40.561876Z",
     "shell.execute_reply.started": "2024-02-02T07:40:29.597459Z",
     "to_execute": "2024-02-02T07:40:29.542Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.87s/it]\n"
     ]
    }
   ],
   "source": [
    "class ARGS:\n",
    "    def __init__(self):\n",
    "        self.subject = 'S1'\n",
    "        self.gpt = 'perceived'\n",
    "        self.sessions = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 18, 20]\n",
    "        self.layer = 17\n",
    "        self.layer2 = 18\n",
    "        self.act_name = 'ffn_gate'\n",
    "        self.window = 15\n",
    "        self.chunk = 4\n",
    "\n",
    "args = ARGS()\n",
    "\n",
    "model_dir = '/ossfs/workspace/nas/gzhch/data/models/Llama-2-7b-hf'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir, \n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16,\n",
    ").eval()\n",
    "\n",
    "# model = None\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "## load cached llm act if possible\n",
    "cache_dir = '/ossfs/workspace/nas/gzhch/data/cache'\n",
    "llama = LLAMA(model, tokenizer, cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27da0c2e-441d-4f5c-9429-96297271cef3",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-02-02T07:41:04.728782Z",
     "shell.execute_reply.started": "2024-02-02T07:41:04.619531Z",
     "to_execute": "2024-02-02T07:41:04.558Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "def load_data(task_name, n_shot=1, seed=42):\n",
    "    data_dirs = {\n",
    "        'xsum' : '/ossfs/workspace/nas/gzhch/data/datasets/xsum',\n",
    "        'gsm8k' : '/ossfs/workspace/nas/gzhch/data/datasets/gsm8k',\n",
    "        'alpaca' : '/ossfs/workspace/nas/gzhch/data/datasets/alpaca',\n",
    "        'wmt' : '/ossfs/workspace/nas/gzhch/data/datasets/wmt14_de-en_test',\n",
    "        'wikitext2' : '/ossfs/workspace/nas/gzhch/data/datasets/wikitext-2-v1',\n",
    "        'wikitext_dense' : '/ossfs/workspace/nas/gzhch/data/datasets/wikitext-2-v1',\n",
    "        'wikitext_eval' : '/ossfs/workspace/nas/gzhch/data/datasets/wikitext-2-v1',\n",
    "        'cross_language' : '/ossfs/workspace/nas/gzhch/data/datasets/wmt14_de-en_test',\n",
    "    }\n",
    "    if task_name == 'gsm8k':\n",
    "        dataset = datasets.load_dataset(data_dirs[task_name])\n",
    "    elif task_name == 'wikitext2':\n",
    "        dataset = datasets.load_from_disk(data_dirs[task_name])\n",
    "        dataset = dataset['train'].filter(lambda x: len(x['text'])>100) \n",
    "        dataset = dataset.select(random.sample(range(len(dataset)), 1000))\n",
    "\n",
    "    elif task_name == 'wikitext_eval':\n",
    "        dataset = datasets.load_from_disk(data_dirs[task_name])\n",
    "        dataset = dataset['test'].filter(lambda x: len(x['text'])>100) \n",
    "\n",
    "    elif task_name == 'cross_language':\n",
    "        dataset = datasets.load_from_disk(data_dirs[task_name])\n",
    "        de_data = dataset.map(lambda e: dict(text=e['translation']['de']))\n",
    "        en_data = dataset.map(lambda e: dict(text=e['translation']['en']))\n",
    "        return en_data, de_data\n",
    "\n",
    "    elif task_name == 'wikitext_dense':\n",
    "        def tokenize_texts(examples):\n",
    "            tokenized_inputs = tokenizer(examples[\"text\"])\n",
    "            return tokenized_inputs\n",
    "\n",
    "        def group_texts(examples):\n",
    "            # Concatenate all texts.\n",
    "            max_length = 1024\n",
    "            concatenated_examples = {k: list(chain(*examples[k])) for k in ['input_ids']}\n",
    "            total_length = len(concatenated_examples['input_ids'])\n",
    "            # print(len(concatenated_examples['input_ids']), '\\n')\n",
    "            # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "            # customize this part to your needs.\n",
    "            if total_length >= max_length:\n",
    "                total_length = (total_length // max_length) * max_length\n",
    "            # else:\n",
    "                # print('aaa')\n",
    "            # Split by chunks of max_len.\n",
    "            # result = {\n",
    "            #     k: [t[i : i + max_length] for i in range(0, total_length, max_length)]\n",
    "            #     for k, t in concatenated_examples.items()\n",
    "            # }\n",
    "            result = {'input_ids': [concatenated_examples['input_ids'][i : i + max_length] for i in range(0, total_length, max_length)]}\n",
    "            return result\n",
    "\n",
    "        dataset = datasets.load_from_disk(data_dirs[task_name])\n",
    "        dataset = dataset.map(tokenize_texts, batched=True, num_proc=4)\n",
    "        dataset = dataset.map(group_texts, batched=True, num_proc=4, remove_columns=['text', 'attention_mask'])\n",
    "        dataset['train'] = dataset['train'].shuffle(seed=seed)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# 创建一个简单的两层全连接神经网络\n",
    "class Projector(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Projector, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.act = nn.SiLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.act(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.act(out)\n",
    "        return out\n",
    "\n",
    "class LinearProjector(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearProjector, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        # self.act = nn.SiLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        # out = self.act(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval(x, y, net):\n",
    "    output = net(x)\n",
    "    loss = criterion(output, y)\n",
    "    return loss\n",
    "\n",
    "def deduplication(data):\n",
    "    tokens = data['context'][:, 5]\n",
    "    unique_tokens = []\n",
    "    unique_token_ids = []\n",
    "    for idx in range(len(tokens)):\n",
    "        if tokens[idx] not in unique_tokens:\n",
    "            unique_tokens.append(tokens[idx])\n",
    "            unique_token_ids.append(idx)\n",
    "    random.shuffle(unique_token_ids)\n",
    "    ids = unique_token_ids\n",
    "\n",
    "    return {k : v[ids] for k, v in data.items()}\n",
    "\n",
    "def train(net, train_set, stim_neurons=None, resp_neurons=None, max_step=100000):\n",
    "    logs = []\n",
    "    # layer1, layer2 = 10, 15\n",
    "    total_batch = len(train_set) // batch_size\n",
    "\n",
    "    total_batch = min(total_batch, max_step)\n",
    "    for b in range(total_batch):\n",
    "        input_ids = train_set[b * batch_size: (b + 1) * batch_size]['input_ids']\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        input = dict(input_ids=input_ids, attention_mask=torch.ones(input_ids.shape))\n",
    "        with torch.no_grad():\n",
    "            res = llama.get_neuron_activation_and_loss(input)\n",
    "\n",
    "        if stim_neurons is not None:\n",
    "            X = res['ffn_gate'][:, layer1, stim_neurons].cuda().float()\n",
    "        else:\n",
    "            X = res['ffn_gate'][:, layer1, :].cuda().float()\n",
    "        if resp_neurons is not None:\n",
    "            Y = res['ffn_gate'][:, layer2, resp_neurons].cuda().float()\n",
    "        else:\n",
    "            Y = res['ffn_gate'][:, layer2, :].cuda().float()\n",
    "\n",
    "        output = net(X)\n",
    "        loss = criterion(output, Y)\n",
    "        \n",
    "        optimizer.zero_grad() \n",
    "        (loss * output.shape[1]).backward()        \n",
    "        optimizer.step()       \n",
    "        \n",
    "        if (b+1) % 1 == 0:\n",
    "            eval_loss = eval(test_X.cuda(), test_Y.cuda(), net).item()\n",
    "            print(f'Epoch [{b+1}/{total_batch}], Train Loss: {loss.item():.6f}, Eval Loss: {eval_loss:.6f}')\n",
    "            logs.append(f'Epoch [{b+1}/{total_batch}], Train Loss: {loss.item():.6f}, Eval Loss: {eval_loss:.6f}')\n",
    "    return logs\n",
    "\n",
    "def evaluate_ppl(eval_data, model, fake_ffn=None, num_of_batch=3, **forwrd_args):\n",
    "    ppls = []\n",
    "    batch_size = 100\n",
    "    for b in range(num_of_batch):\n",
    "        input = tokenizer(eval_data['text'][b * batch_size: (b + 1) * batch_size], padding='longest', return_tensors='pt')\n",
    "        result = ana.custom_forward(model, input['input_ids'].cuda(), inspect_acts=['ffn_gate'], fake_ffn=fake_ffn, **forwrd_args)\n",
    "        logits = result['logits']\n",
    "        labels = input['input_ids']\n",
    "        input_ids = input['input_ids'][:, :-1]\n",
    "\n",
    "        # calculate loss\n",
    "        shift_logits = logits[..., :-1, :].contiguous().view(-1, 32000)\n",
    "        shift_labels = labels[..., 1:].contiguous().view(-1)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(reduce=False)\n",
    "        loss = loss_fct(shift_logits, shift_labels).view(labels.shape[0], -1)\n",
    "        t = (loss * input['attention_mask'][:, :-1]).sum(dim=1)/input['attention_mask'].sum(dim=1)\n",
    "        ppls += torch.exp(t).tolist()\n",
    "    ppl = torch.nan_to_num(torch.tensor(ppls)).mean().tolist()\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abfaab8e-a7f5-4157-8fb6-08374e446bcb",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-02-02T07:41:56.639584Z",
     "shell.execute_reply.started": "2024-02-02T07:41:08.575174Z",
     "to_execute": "2024-02-02T07:41:08.501Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "wiki_data = load_data('wikitext_dense')\n",
    "\n",
    "# get test data once and for all\n",
    "batch_size = 10\n",
    "test_data = []\n",
    "for b in range(5):\n",
    "    input_ids = wiki_data['validation'][b * batch_size: (b + 1) * batch_size]['input_ids']\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    input = dict(input_ids=input_ids, attention_mask=torch.ones(input_ids.shape))\n",
    "    with torch.no_grad():\n",
    "        res = llama.get_neuron_activation_and_loss(input)\n",
    "        test_data.append(res)\n",
    "test_data = {k: torch.cat([i[k] for i in test_data]) for k in test_data[0].keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d37792b-f1c1-4dfe-a08d-08a7488dacb1",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-02-02T07:42:11.255064Z",
     "shell.execute_reply.started": "2024-02-02T07:42:10.981039Z",
     "to_execute": "2024-02-02T07:42:10.907Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "def get_log(layer1, layer2):\n",
    "\n",
    "    with open(f'/ossfs/workspace/cache_v2/{layer1}-{layer2}.txt', 'r') as f:\n",
    "        logs = f.readlines()\n",
    "    return logs\n",
    "\n",
    "\n",
    "def get_predicted(layer1, layer2):\n",
    "\n",
    "    with open(f'/ossfs/workspace/cache_v2/{layer1}-{layer2}.txt', 'r') as f:\n",
    "        logs = f.readlines()\n",
    "    # return logs\n",
    "\n",
    "    stim_neurons = None\n",
    "    resp_neurons = None\n",
    "\n",
    "    ### get text set\n",
    "    test_X, test_Y = [], []\n",
    "    if stim_neurons is not None:\n",
    "        test_X = test_data['ffn_gate'][:, layer1, stim_neurons].cuda().half()\n",
    "    else:\n",
    "        test_X = test_data['ffn_gate'][:, layer1, :].cuda().half()\n",
    "    if resp_neurons is not None:\n",
    "        test_Y = test_data['ffn_gate'][:, layer2, resp_neurons].cuda().half()\n",
    "    else:\n",
    "        test_Y = test_data['ffn_gate'][:, layer2, :].cuda().half()\n",
    "\n",
    "    save_path = f'/ossfs/workspace/cache_v2/net_{layer1}_{layer2}.pt'\n",
    "    # save_path = f'/ossfs/workspace/nas/gzhch/data/cache/llama-7b/net_{layer1}_{layer2}.pt'\n",
    "    # if not os.path.exists(save_path):\n",
    "    #     save_path = f'/ossfs/workspace/nas/gzhch/data/cache/llama-7b/net_{layer1}_{layer2}.pt'\n",
    "\n",
    "    net = torch.load(save_path).half()\n",
    "\n",
    "    pred = net(test_X)\n",
    "\n",
    "    th = 0.6\n",
    "    pred = net(test_X)\n",
    "    delta = pred - test_Y\n",
    "    ids = torch.nonzero(((delta.std(dim=0)) / test_Y.std(dim=0)).abs() < th).squeeze()\n",
    "    return pred, test_Y, logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ead68b98-111d-4763-b299-83cd7c246fb5",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-02-02T10:48:54.829490Z",
     "shell.execute_reply.started": "2024-02-02T10:28:19.418039Z",
     "to_execute": "2024-02-02T10:28:19.290Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "# for layer1, layer2 in product(range(0, 32, 2), range(0, 32, 2)):\n",
    "# for layer1 in range(0, 32-2, 2):\n",
    "for layer1 in range(0, 31, 1):\n",
    "    layer2 = layer1 + 1\n",
    "    pred, test_Y, _ = get_predicted(layer1, layer2)\n",
    "    neuron_pearson = []\n",
    "    for i in range(pred.shape[1]):\n",
    "        stat = scipy.stats.pearsonr(pred[:, i].cpu().detach(), test_Y[:, i].cpu().detach())\n",
    "        neuron_pearson.append(stat.statistic)\n",
    "    neuron_pearson = torch.tensor(neuron_pearson).half()\n",
    "    neuron_std = (pred - test_Y).std(dim=0).cpu()\n",
    "    with open(f'/ossfs/workspace/cache_v2/neuron_statistic_llama7b/{layer1}_{layer2}.pkl', 'wb') as f:\n",
    "        result = dict(pearson=neuron_pearson, l2=neuron_std, Y_mean=test_Y.mean(dim=0), Y_std=test_Y.std(dim=0))\n",
    "        pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "342d7751-374f-4cc1-a122-04d7187ce689",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-02-02T10:48:54.870060Z",
     "shell.execute_reply.started": "2024-02-02T10:48:54.832311Z",
     "to_execute": "2024-02-02T10:48:54.719Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "1 2\n",
      "2 3\n",
      "3 4\n",
      "4 5\n",
      "5 6\n",
      "\n",
      "\n",
      "8 9\n",
      "9 10\n",
      "10 11\n",
      "11 12\n",
      "12 13\n",
      "13 14\n",
      "\n",
      "\n",
      "16 17\n",
      "17 18\n",
      "18 19\n",
      "19 20\n",
      "20 21\n",
      "21 22\n",
      "\n",
      "\n",
      "24 25\n",
      "25 26\n",
      "26 27\n",
      "27 28\n",
      "28 29\n",
      "29 30\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "for layer1 in range(0, 32, block_size):\n",
    "    for layer2 in range(layer1 + 1, layer1 + block_size - 1, 1):\n",
    "        print(layer2 - 1, layer2)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "dacfe2b1-5d2e-4f50-a6ca-372021e0bdaf",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-02-01T13:19:13.190853Z",
     "shell.execute_reply.started": "2024-02-01T13:19:13.049642Z",
     "to_execute": "2024-02-01T13:19:13.004Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "filters = {}\n",
    "for stat_name in neuron_stat.keys():\n",
    "    stat = neuron_stat[stat_name].float()\n",
    "    up_half = stat > stat.median()\n",
    "    down_half = ~up_half\n",
    "    filters[f'{stat_name}_up'] = up_half\n",
    "    filters[f'{stat_name}_down'] = down_half\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "f22f6190-030d-4a20-8e1d-49d6bd52ba35",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-02-01T13:40:41.049953Z",
     "shell.execute_reply.started": "2024-02-01T13:40:41.020515Z",
     "to_execute": "2024-02-01T13:40:40.998Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "t = test_Y * list(filters.values())[0].to(test_Y.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "ca90b6bc-84ef-4030-b6d2-1dc9ea9298f2",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-02-01T13:40:43.988763Z",
     "shell.execute_reply.started": "2024-02-01T13:40:43.958513Z",
     "to_execute": "2024-02-01T13:40:43.937Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([51150, 11008])"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "284552f9-ba4d-42ff-8c70-f073d79ea308",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-02-01T08:11:36.881131Z",
     "shell.execute_reply.started": "2024-02-01T08:11:31.643248Z",
     "to_execute": "2024-02-01T08:11:31.597Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[503], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer1 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m2\u001b[39m)):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j, layer2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m2\u001b[39m)):\n\u001b[0;32m----> 5\u001b[0m         logs \u001b[38;5;241m=\u001b[39m \u001b[43mget_log\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m         results[i]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(logs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n",
      "Cell \u001b[0;32mIn[502], line 3\u001b[0m, in \u001b[0;36mget_log\u001b[0;34m(layer1, layer2)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_log\u001b[39m(layer1, layer2):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/ossfs/workspace/cache_v2/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlayer1\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlayer2\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m         logs \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/_bootlocale.py:33\u001b[0m, in \u001b[0;36mgetpreferredencoding\u001b[0;34m(do_setlocale)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m locale\u001b[38;5;241m.\u001b[39mgetpreferredencoding(do_setlocale)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetpreferredencoding\u001b[39m(do_setlocale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m do_setlocale\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mutf8_mode:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## get log\n",
    "results = [[] for _ in range(0, 32, 2)]\n",
    "for i, layer1 in enumerate(range(0, 32, 2)):\n",
    "    for j, layer2 in enumerate(range(0, 32, 2)):\n",
    "        logs = get_log(layer1, layer2)\n",
    "        results[i].append(float(logs[-1].split()[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "679b938d-69cc-4cc7-aae2-78bacfa1ddd3",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-02-01T08:31:26.740360Z",
     "shell.execute_reply.started": "2024-02-01T08:30:59.894917Z",
     "to_execute": "2024-02-01T08:30:59.848Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "layer1 = 22\n",
    "layer2 = 24\n",
    "pred, test_Y, _ = f(layer1, layer2)\n",
    "\n",
    "neuron_pearson = []\n",
    "for i in range(pred.shape[1]):\n",
    "    stat = scipy.stats.pearsonr(pred[:, i].cpu().detach(), test_Y[:, i].cpu().detach())\n",
    "    neuron_pearson.append(stat.statistic)\n",
    "neuron_pearson = torch.tensor(neuron_pearson)\n",
    "\n",
    "neuron_std = (pred - test_Y).std(dim=0).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "5265c1b0-a76b-456c-97a4-ce1d2e75aaac",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-01-31T12:57:55.580314Z",
     "shell.execute_reply.started": "2024-01-31T12:57:55.548915Z",
     "to_execute": "2024-01-31T12:57:55.392Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "# indices = neuron_pearson.topk(100, largest=True).indices.cpu()\n",
    "indices = neuron_std.topk(10, largest=True).indices.cpu()\n",
    "neuron_id = indices\n",
    "neuron_weight = model.model.layers[layer2].mlp.down_proj.weight[:, neuron_id]\n",
    "lm_head = model.lm_head.weight\n",
    "logit_contribution = torch.matmul(lm_head, neuron_weight.to(lm_head.device)).transpose(0, 1)\n",
    "logits = logit_contribution.topk(100, dim=1).indices\n",
    "# tokenizer.convert_ids_to_tokens(logit_contribution.topk(10, dim=0).indices.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "f3139bc7-d611-47bc-bb99-58c4a5acb891",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-01-31T12:57:56.301675Z",
     "shell.execute_reply.started": "2024-01-31T12:57:56.270891Z",
     "to_execute": "2024-01-31T12:57:56.111Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "976"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "5b9694fb-3222-4629-b61c-c96d0e314e64",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-02-01T08:27:06.007969Z",
     "shell.execute_reply.started": "2024-02-01T08:27:05.969236Z",
     "to_execute": "2024-02-01T08:27:05.927Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([51150, 11008])\n"
     ]
    }
   ],
   "source": [
    "layer = 20\n",
    "neurons = test_data['ffn_gate'][:, layer, :]\n",
    "print(neurons.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "95753eb0-4bd2-4bb9-afab-1ce8580ff531",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-02-01T08:27:16.026738Z",
     "shell.execute_reply.started": "2024-02-01T08:27:15.892822Z",
     "to_execute": "2024-02-01T08:27:15.849Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "token = test_data['context'][:, 4]\n",
    "unique_token_id = Counter(token.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "0f013d62-85ac-46e9-af3b-90dca304ee04",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-02-01T08:31:37.937247Z",
     "shell.execute_reply.started": "2024-02-01T08:31:37.712651Z",
     "to_execute": "2024-02-01T08:31:37.669Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token:  token_id: 29871 count: 5777\n",
      "neuron_count: 48\n",
      "tensor(0.3103, dtype=torch.float64) tensor(0.1316, dtype=torch.float64)\n",
      "tensor(0.2876, dtype=torch.float64) tensor(0.0995, dtype=torch.float64)\n",
      "tensor(0.1432, dtype=torch.float16, grad_fn=<MeanBackward0>) tensor(0.0427, dtype=torch.float16, grad_fn=<StdBackward0>)\n",
      "tensor(0.1321, dtype=torch.float16, grad_fn=<MeanBackward0>) tensor(0.0329, dtype=torch.float16, grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "id, count = unique_token_id.most_common()[0]\n",
    "token_neurons = neurons[token == id]\n",
    "print('token:', tokenizer.decode(id), 'token_id:', id, 'count:', count)\n",
    "# avg_token_neurons = token_neurons.mean(dim=0)\n",
    "# ids = (token_neurons>0.5).sum(dim=0).topk(1).indices\n",
    "\n",
    "ids = torch.nonzero((token_neurons.abs()>0.3).sum(dim=0) > count*0.2)\n",
    "print('neuron_count:', len(ids))\n",
    "print(neuron_pearson[ids].mean(), neuron_pearson[ids].std())\n",
    "print(neuron_pearson.mean(), neuron_pearson.std())\n",
    "print(neuron_std[ids].mean(), neuron_std[ids].std())\n",
    "print(neuron_std.mean(), neuron_std.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "fcafe59e-70ad-4fcd-a2ab-2e8db95c3537",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-02-01T08:30:49.575672Z",
     "shell.execute_reply.started": "2024-02-01T08:30:48.857481Z",
     "to_execute": "2024-02-01T08:30:48.814Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuron_count: 51\n",
      "tensor(0.2373, dtype=torch.float64) tensor(0.0713, dtype=torch.float64)\n",
      "tensor(0.2259, dtype=torch.float64) tensor(0.0695, dtype=torch.float64)\n",
      "tensor(0.1271, dtype=torch.float16, grad_fn=<MeanBackward0>) tensor(0.0395, dtype=torch.float16, grad_fn=<StdBackward0>)\n",
      "tensor(0.1250, dtype=torch.float16, grad_fn=<MeanBackward0>) tensor(0.0432, dtype=torch.float16, grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# random_mask = torch.rand(neurons.shape[0])>0.9999\n",
    "count = 100\n",
    "random_mask = random.sample(range(neurons.shape[0]), count)\n",
    "token_neurons = neurons[random_mask]\n",
    "# ids = torch.nonzero((token_neurons.abs()>0.3).sum(dim=0) > count*0.1)\n",
    "ids = torch.nonzero((neurons.abs()>1).sum(dim=0)>1000)\n",
    "print('neuron_count:', len(ids))\n",
    "print(neuron_pearson[ids].mean(), neuron_pearson[ids].std())\n",
    "print(neuron_pearson.mean(), neuron_pearson.std())\n",
    "print(neuron_std[ids].mean(), neuron_std[ids].std())\n",
    "print(neuron_std.mean(), neuron_std.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "92208771-3658-4d4a-b719-1b44c962e562",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-02-01T08:30:34.746911Z",
     "shell.execute_reply.started": "2024-02-01T08:30:34.029709Z",
     "to_execute": "2024-02-01T08:30:33.981Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([20414, 16531,  8489,  6020,  4658,  3533,  3358,  2221,  2163,  2014,\n",
       "         1919,  1885,  1878,  1841,  1787,  1762,  1756,  1647,  1630,  1534,\n",
       "         1465,  1383,  1346,  1328,  1326,  1319,  1316,  1279,  1259,  1255,\n",
       "         1236,  1234,  1232,  1225,  1196,  1196,  1190,  1176,  1166,  1165,\n",
       "         1154,  1154,  1121,  1118,  1113,  1070,  1046,  1038,  1032,  1031,\n",
       "         1030,   999,   999,   995,   980,   970,   966,   960,   941,   938,\n",
       "          936,   928,   923,   923,   923,   914,   911,   904,   903,   879,\n",
       "          859,   859,   854,   845,   843,   840,   837,   833,   830,   821,\n",
       "          819,   817,   804,   800,   799,   789,   788,   770,   755,   752,\n",
       "          746,   743,   735,   732,   731,   728,   726,   725,   724,   724]),\n",
       "indices=tensor([ 9851,  4752,  2018,  8771,  9750,  4420, 10606,  7525,  1068,  5708,\n",
       "          403,  1900,  3230,  9115,  5469,  2579,  5026,  9315,  1830, 10318,\n",
       "         9671,  8074,  9202,  3983,  5712,  1701,  5507,  9532,  9954,  2366,\n",
       "         9932,  2196, 10553,  1071,  9441, 10954,  3588,  8933,  4935,  5124,\n",
       "         2644,  4277,  7139,   387,  6006,  4478,  4203,  1467,  6447,  8745,\n",
       "         7276,  9083,  4386,  8559,  1882,  1702,  1906,  6435, 10155,  2148,\n",
       "         4694,  3389,  1967,  6357, 10660,  8010, 10040,  9100, 10492,  4272,\n",
       "         2776,  3281,  1170,  5431,  7448, 10574,  4008,   673,  2507,  4289,\n",
       "         7214, 10369,   488,  2476,  3452,  9429,  9719,  2786,  1698,  4972,\n",
       "         4167,  9458,  2761,  7740,  8777,  4918,  2749,  9509,  5299,  3488]))"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(neurons.abs()>1).sum(dim=0).topk(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "efa9c08e-d5bf-4332-9d94-2c148fb4cb8e",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-02-01T08:05:55.460161Z",
     "shell.execute_reply.started": "2024-02-01T08:05:55.428811Z",
     "to_execute": "2024-02-01T08:05:55.383Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10186)"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.rand(neurons.shape[0])>0.8).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "9c880746-d273-4c7b-8c0b-57a82ef16ff1",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-01-31T12:23:41.899593Z",
     "shell.execute_reply.started": "2024-01-31T12:22:57.756473Z",
     "to_execute": "2024-01-31T12:22:57.672Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "layer1 = 4\n",
    "layer2 = 6\n",
    "en_data, de_data = load_data('cross_language')\n",
    "eval_data = en_data\n",
    "fake_ffn = ana.FFNProjector(layer1, layer2, torch.load(f'/ossfs/workspace/cache_v2/net_{layer1}_{layer2}.pt'))\n",
    "\n",
    "batch = [9]\n",
    "ppls = []\n",
    "batch_size = 100\n",
    "for b in range(10):\n",
    "    input = tokenizer(eval_data['text'][b * batch_size: (b + 1) * batch_size], padding='longest', return_tensors='pt')\n",
    "    result = ana.custom_forward(model, input['input_ids'].cuda(), inspect_acts=['ffn_gate'], fake_ffn=fake_ffn)\n",
    "    logits = result['logits']\n",
    "    labels = input['input_ids']\n",
    "    input_ids = input['input_ids'][:, :-1]\n",
    "\n",
    "    # calculate loss\n",
    "    shift_logits = logits[..., :-1, :].contiguous().view(-1, 32000)\n",
    "    shift_labels = labels[..., 1:].contiguous().view(-1)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(reduce=False)\n",
    "    loss = loss_fct(shift_logits, shift_labels).view(labels.shape[0], -1)\n",
    "    # print(loss)\n",
    "    t = (loss * input['attention_mask'][:, :-1]).sum(dim=1)/input['attention_mask'].sum(dim=1)\n",
    "    ppls += torch.exp(t).tolist()\n",
    "ppl = torch.nan_to_num(torch.tensor(ppls)).mean().tolist()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "5b7eba6b-e0af-4063-8846-dfe56cd1ad7d",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-02-01T09:20:37.060619Z",
     "shell.execute_reply.started": "2024-02-01T09:20:37.030011Z",
     "to_execute": "2024-02-01T09:20:32.923Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type([1])==list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "c670787e-cb42-4002-bacf-627207095ec1",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-02-01T10:53:16.575321Z",
     "shell.execute_reply.started": "2024-02-01T10:53:13.444701Z",
     "to_execute": "2024-02-01T10:53:11.159Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2\n",
      "2 4\n",
      "4 6\n",
      "\n",
      "\n",
      "8 10\n",
      "10 12\n",
      "12 14\n",
      "\n",
      "\n",
      "16 18\n",
      "18 20\n",
      "20 22\n",
      "\n",
      "\n",
      "24 26\n",
      "26 28\n",
      "28 30\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "\n",
    "for layer1 in range(0, 32, block_size):\n",
    "    for layer2 in range(layer1+2, layer1 + block_size - 1, 2):\n",
    "        print(layer2-2, layer2)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23d0b051-0d38-4b45-8d64-6159a3cae6f7",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2024-02-02T07:00:06.009723Z",
     "shell.execute_reply.started": "2024-02-02T07:00:06.007232Z",
     "to_execute": "2024-02-02T07:00:05.949Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3\n",
      "3 5\n",
      "5 7\n",
      "7 9\n",
      "9 11\n",
      "11 13\n",
      "13 15\n",
      "15 17\n",
      "17 19\n",
      "19 21\n",
      "21 23\n",
      "23 25\n",
      "25 27\n",
      "27 29\n",
      "29 31\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793e6035-285f-4eca-aad8-ba31d0ebebea",
   "metadata": {
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
